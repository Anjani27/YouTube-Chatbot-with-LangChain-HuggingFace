{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f65f8ae1-d29b-4132-b20c-fa116b7432dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "llmm = HuggingFaceEndpoint(\n",
    "    repo_id= \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", #\"google/gemma-2-2b-it\",\n",
    "    task=\"text-generation\"\n",
    ")\n",
    "model = ChatHuggingFace(llm = llmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f149b3-4dc3-4c98-b876-7fae6092533a",
   "metadata": {},
   "source": [
    "# Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "27aeaf11-9bef-4ab7-84e1-8e525665cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q youtube-transcript-api langchain-community langchain-openai \\ faiss-cpu tiktoken python-detenv\n",
    "#!pip install langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aff87741-2c9b-4417-9636-fc9834c03182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS \n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62239e9c-b670-4b68-950a-3c7cb998944a",
   "metadata": {},
   "source": [
    "### Step 1a - Indexing(Document Ingestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9bcf45b5-96f0-4098-8fea-f00e9abab8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine you happen across a short movie script that describes a scene between a person and their AI assistant. The script has what the person asks the AI, but the AI's response has been torn off. Suppose you also have this powerful magical machine that can take any text and provide a sensible prediction of what word comes next. You could then finish the script by feeding in what you have to the machine, seeing what it would predict to start the AI's answer, and then repeating this over and over with a growing script completing the dialogue. When you interact with a chatbot, this is exactly what's happening. A large language model is a sophisticated mathematical function that predicts what word comes next for any piece of text. Instead of predicting one word with certainty, though, what it does is assign a probability to all possible next words. To build a chatbot, you lay out some text that describes an interaction between a user and a hypothetical AI assistant, add on whatever the user types in as the first part of the interaction, and then have the model repeatedly predict the next word that such a hypothetical AI assistant would say in response, and that's what's presented to the user. In doing this, the output tends to look a lot more natural if you allow it to select less likely words along the way at random. So what this means is even though the model itself is deterministic, a given prompt typically gives a different answer each time it's run. Models learn how to make these predictions by processing an enormous amount of text, typically pulled from the internet. For a standard human to read the amount of text that was used to train GPT-3, for example, if they read non-stop 24-7, it would take over 2600 years. Larger models since then train on much, much more. You can think of training a little bit like tuning the dials on a big machine. The way that a language model behaves is entirely determined by these many different continuous values, usually called parameters or weights. Changing those parameters will change the probabilities that the model gives for the next word on a given input. What puts the large in large language model is how they can have hundreds of billions of these parameters. No human ever deliberately sets those parameters. Instead, they begin at random, meaning the model just outputs gibberish, but they're repeatedly refined based on many example pieces of text. One of these training examples could be just a handful of words, or it could be thousands, but in either case, the way this works is to pass in all but the last word from that example into the model and compare the prediction that it makes with the true last word from the example. An algorithm called backpropagation is used to tweak all of the parameters in such a way that it makes the model a little more likely to choose the true last word and a little less likely to choose all the others. When you do this for many, many trillions of examples, not only does the model start to give more accurate predictions on the training data, but it also starts to make more reasonable predictions on text that it's never seen before. Given the huge number of parameters and the enormous amount of training data, the scale of computation involved in training a large language model is mind-boggling. To illustrate, imagine that you could perform one billion additions and multiplications every single second. How long do you think it would take for you to do all of the operations involved in training the largest language models? Do you think it would take a year? Maybe something like 10,000 years? The answer is actually much more than that. It's well over 100 million years. This is only part of the story, though. This whole process is called pre-training. The goal of auto-completing a random passage of text from the internet is very different from the goal of being a good AI assistant. To address this, chatbots undergo another type of training, just as important, called reinforcement learning with human feedback. Workers flag unhelpful or problematic predictions, and their corrections further change the model's parameters, making them more likely to give predictions that users prefer. Looking back at the pre-training, though, this staggering amount of computation is only made possible by using special computer chips that are optimized for running many operations in parallel, known as GPUs. However, not all language models can be easily parallelized. Prior to 2017, most language models would process text one word at a time, but then a team of researchers at Google introduced a new model known as the transformer. Transformers don't read text from the start to the finish, they soak it all in at once, in parallel. The very first step inside a transformer, and most other language models for that matter, is to associate each word with a long list of numbers. The reason for this is that the training process only works with continuous values, so you have to somehow encode language using numbers, and each of these lists of numbers may somehow encode the meaning of the corresponding word. What makes transformers unique is their reliance on a special operation known as attention. This operation gives all of these lists of numbers a chance to talk to one another and refine the meanings they encode based on the context around, all done in parallel. For example, the numbers encoding the word bank might be changed based on the context surrounding it to somehow encode the more specific notion of a riverbank. Transformers typically also include a second type of operation known as a feed-forward neural network, and this gives the model extra capacity to store more patterns about language learned during training. All of this data repeatedly flows through many different iterations of these two fundamental operations, and as it does so, the hope is that each list of numbers is enriched to encode whatever information might be needed to make an accurate prediction of what word follows in the passage. At the end, one final function is performed on the last vector in this sequence, which now has had a chance to be influenced by all the other context from the input text, as well as everything the model learned during training, to produce a prediction of the next word. Again, the model's prediction looks like a probability for every possible next word. Although researchers design the framework for how each of these steps work, it's important to understand that the specific behavior is an emergent phenomenon based on how those hundreds of billions of parameters are tuned during training. This makes it incredibly challenging to determine why the model makes the exact predictions that it does. What you can see is that when you use large language model predictions to autocomplete a prompt, the words that it generates are uncannily fluent, fascinating, and even useful. If you're a new viewer and you're curious about more details on how transformers and attention work, boy do I have some material for you. One option is to jump into a series I made about deep learning, where we visualize and motivate the details of attention and all the other steps in a transformer. Also, on my second channel I just posted a talk I gave a couple months ago about this topic for the company TNG in Munich. Sometimes I actually prefer the content I make as a casual talk rather than a produced video, but I leave it up to you which one of these feels like the better follow-on.\n"
     ]
    }
   ],
   "source": [
    "video_id = \"LPZh9BOjkQs\" #only the ID, not full URL\n",
    "try:\n",
    "    # If you don't care which language, this returns the \"best\" one\n",
    "    transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=[\"en\"])\n",
    "\n",
    "    # Flatten it to plain text \n",
    "    transcript = \" \".join(chunk[\"text\"] for chunk in transcript_list)\n",
    "    print(transcript)\n",
    "\n",
    "except TranscriptsDisabled:\n",
    "    print(\"No captions available for this video.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3c5f674f-daaf-4192-89f2-5e67be69537e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Imagine you happen across a short movie script that',\n",
       "  'start': 1.14,\n",
       "  'duration': 2.836},\n",
       " {'text': 'describes a scene between a person and their AI assistant.',\n",
       "  'start': 3.976,\n",
       "  'duration': 3.164},\n",
       " {'text': \"The script has what the person asks the AI, but the AI's response has been torn off.\",\n",
       "  'start': 7.48,\n",
       "  'duration': 5.58},\n",
       " {'text': 'Suppose you also have this powerful magical machine that can take',\n",
       "  'start': 13.06,\n",
       "  'duration': 3.92},\n",
       " {'text': 'any text and provide a sensible prediction of what word comes next.',\n",
       "  'start': 16.98,\n",
       "  'duration': 3.98},\n",
       " {'text': 'You could then finish the script by feeding in what you have to the machine,',\n",
       "  'start': 21.5,\n",
       "  'duration': 4.006},\n",
       " {'text': \"seeing what it would predict to start the AI's answer,\",\n",
       "  'start': 25.506,\n",
       "  'duration': 2.862},\n",
       " {'text': 'and then repeating this over and over with a growing script completing the dialogue.',\n",
       "  'start': 28.368,\n",
       "  'duration': 4.372},\n",
       " {'text': \"When you interact with a chatbot, this is exactly what's happening.\",\n",
       "  'start': 33.38,\n",
       "  'duration': 3.1},\n",
       " {'text': 'A large language model is a sophisticated mathematical function',\n",
       "  'start': 37.02,\n",
       "  'duration': 3.681},\n",
       " {'text': 'that predicts what word comes next for any piece of text.',\n",
       "  'start': 40.701,\n",
       "  'duration': 3.279},\n",
       " {'text': 'Instead of predicting one word with certainty, though,',\n",
       "  'start': 44.38,\n",
       "  'duration': 3.022},\n",
       " {'text': 'what it does is assign a probability to all possible next words.',\n",
       "  'start': 47.402,\n",
       "  'duration': 3.518},\n",
       " {'text': 'To build a chatbot, you lay out some text that describes an interaction between a user',\n",
       "  'start': 51.62,\n",
       "  'duration': 5.18},\n",
       " {'text': 'and a hypothetical AI assistant, add on whatever the user types in as the first part of',\n",
       "  'start': 56.8,\n",
       "  'duration': 5.24},\n",
       " {'text': 'the interaction, and then have the model repeatedly predict the next word that such a',\n",
       "  'start': 62.04,\n",
       "  'duration': 5.12},\n",
       " {'text': \"hypothetical AI assistant would say in response, and that's what's presented to the user.\",\n",
       "  'start': 67.16,\n",
       "  'duration': 5.3},\n",
       " {'text': 'In doing this, the output tends to look a lot more natural if',\n",
       "  'start': 73.08,\n",
       "  'duration': 3.134},\n",
       " {'text': 'you allow it to select less likely words along the way at random.',\n",
       "  'start': 76.214,\n",
       "  'duration': 3.286},\n",
       " {'text': 'So what this means is even though the model itself is deterministic,',\n",
       "  'start': 80.14,\n",
       "  'duration': 3.48},\n",
       " {'text': \"a given prompt typically gives a different answer each time it's run.\",\n",
       "  'start': 83.62,\n",
       "  'duration': 3.48},\n",
       " {'text': 'Models learn how to make these predictions by processing an enormous amount of text,',\n",
       "  'start': 88.04,\n",
       "  'duration': 4.292},\n",
       " {'text': 'typically pulled from the internet.',\n",
       "  'start': 92.332,\n",
       "  'duration': 1.768},\n",
       " {'text': 'For a standard human to read the amount of text that was used to train GPT-3,',\n",
       "  'start': 94.1,\n",
       "  'duration': 5.371},\n",
       " {'text': 'for example, if they read non-stop 24-7, it would take over 2600 years.',\n",
       "  'start': 99.471,\n",
       "  'duration': 4.889},\n",
       " {'text': 'Larger models since then train on much, much more.',\n",
       "  'start': 104.72,\n",
       "  'duration': 2.62},\n",
       " {'text': 'You can think of training a little bit like tuning the dials on a big machine.',\n",
       "  'start': 108.2,\n",
       "  'duration': 3.58},\n",
       " {'text': 'The way that a language model behaves is entirely determined by these',\n",
       "  'start': 112.28,\n",
       "  'duration': 4.021},\n",
       " {'text': 'many different continuous values, usually called parameters or weights.',\n",
       "  'start': 116.301,\n",
       "  'duration': 4.079},\n",
       " {'text': 'Changing those parameters will change the probabilities',\n",
       "  'start': 121.02,\n",
       "  'duration': 3.079},\n",
       " {'text': 'that the model gives for the next word on a given input.',\n",
       "  'start': 124.099,\n",
       "  'duration': 3.081},\n",
       " {'text': 'What puts the large in large language model is how',\n",
       "  'start': 127.86,\n",
       "  'duration': 2.867},\n",
       " {'text': 'they can have hundreds of billions of these parameters.',\n",
       "  'start': 130.727,\n",
       "  'duration': 3.093},\n",
       " {'text': 'No human ever deliberately sets those parameters.',\n",
       "  'start': 135.2,\n",
       "  'duration': 2.84},\n",
       " {'text': 'Instead, they begin at random, meaning the model just outputs gibberish,',\n",
       "  'start': 138.44,\n",
       "  'duration': 4.203},\n",
       " {'text': \"but they're repeatedly refined based on many example pieces of text.\",\n",
       "  'start': 142.643,\n",
       "  'duration': 3.917},\n",
       " {'text': 'One of these training examples could be just a handful of words,',\n",
       "  'start': 147.14,\n",
       "  'duration': 3.516},\n",
       " {'text': 'or it could be thousands, but in either case, the way this works is to',\n",
       "  'start': 150.656,\n",
       "  'duration': 3.84},\n",
       " {'text': 'pass in all but the last word from that example into the model and',\n",
       "  'start': 154.496,\n",
       "  'duration': 3.624},\n",
       " {'text': 'compare the prediction that it makes with the true last word from the example.',\n",
       "  'start': 158.12,\n",
       "  'duration': 4.22},\n",
       " {'text': 'An algorithm called backpropagation is used to tweak all of the parameters',\n",
       "  'start': 163.26,\n",
       "  'duration': 4.133},\n",
       " {'text': 'in such a way that it makes the model a little more likely to choose',\n",
       "  'start': 167.393,\n",
       "  'duration': 3.803},\n",
       " {'text': 'the true last word and a little less likely to choose all the others.',\n",
       "  'start': 171.196,\n",
       "  'duration': 3.804},\n",
       " {'text': 'When you do this for many, many trillions of examples,',\n",
       "  'start': 175.74,\n",
       "  'duration': 3.01},\n",
       " {'text': 'not only does the model start to give more accurate predictions on the training data,',\n",
       "  'start': 178.75,\n",
       "  'duration': 4.708},\n",
       " {'text': \"but it also starts to make more reasonable predictions on text that it's never\",\n",
       "  'start': 183.458,\n",
       "  'duration': 4.325},\n",
       " {'text': 'seen before.', 'start': 187.783, 'duration': 0.657},\n",
       " {'text': 'Given the huge number of parameters and the enormous amount of training data,',\n",
       "  'start': 189.42,\n",
       "  'duration': 4.499},\n",
       " {'text': 'the scale of computation involved in training a large language model is mind-boggling.',\n",
       "  'start': 193.919,\n",
       "  'duration': 4.961},\n",
       " {'text': 'To illustrate, imagine that you could perform one',\n",
       "  'start': 199.6,\n",
       "  'duration': 2.685},\n",
       " {'text': 'billion additions and multiplications every single second.',\n",
       "  'start': 202.285,\n",
       "  'duration': 3.115},\n",
       " {'text': 'How long do you think it would take for you to do all of the',\n",
       "  'start': 206.06,\n",
       "  'duration': 3.266},\n",
       " {'text': 'operations involved in training the largest language models?',\n",
       "  'start': 209.326,\n",
       "  'duration': 3.214},\n",
       " {'text': 'Do you think it would take a year?',\n",
       "  'start': 213.46,\n",
       "  'duration': 1.579},\n",
       " {'text': 'Maybe something like 10,000 years?',\n",
       "  'start': 216.039,\n",
       "  'duration': 1.921},\n",
       " {'text': 'The answer is actually much more than that.',\n",
       "  'start': 219.02,\n",
       "  'duration': 1.78},\n",
       " {'text': \"It's well over 100 million years.\",\n",
       "  'start': 221.12,\n",
       "  'duration': 2.78},\n",
       " {'text': 'This is only part of the story, though.',\n",
       "  'start': 225.52,\n",
       "  'duration': 1.84},\n",
       " {'text': 'This whole process is called pre-training.',\n",
       "  'start': 227.54,\n",
       "  'duration': 1.68},\n",
       " {'text': 'The goal of auto-completing a random passage of text from the',\n",
       "  'start': 229.5,\n",
       "  'duration': 3.146},\n",
       " {'text': 'internet is very different from the goal of being a good AI assistant.',\n",
       "  'start': 232.646,\n",
       "  'duration': 3.554},\n",
       " {'text': 'To address this, chatbots undergo another type of training,',\n",
       "  'start': 236.88,\n",
       "  'duration': 3.2},\n",
       " {'text': 'just as important, called reinforcement learning with human feedback.',\n",
       "  'start': 240.08,\n",
       "  'duration': 3.68},\n",
       " {'text': 'Workers flag unhelpful or problematic predictions,',\n",
       "  'start': 244.48,\n",
       "  'duration': 3.018},\n",
       " {'text': \"and their corrections further change the model's parameters,\",\n",
       "  'start': 247.498,\n",
       "  'duration': 3.611},\n",
       " {'text': 'making them more likely to give predictions that users prefer.',\n",
       "  'start': 251.109,\n",
       "  'duration': 3.671},\n",
       " {'text': 'Looking back at the pre-training, though, this staggering amount of',\n",
       "  'start': 254.78,\n",
       "  'duration': 4.08},\n",
       " {'text': 'computation is only made possible by using special computer chips that',\n",
       "  'start': 258.86,\n",
       "  'duration': 4.26},\n",
       " {'text': 'are optimized for running many operations in parallel, known as GPUs.',\n",
       "  'start': 263.12,\n",
       "  'duration': 4.14},\n",
       " {'text': 'However, not all language models can be easily parallelized.',\n",
       "  'start': 268.12,\n",
       "  'duration': 3.5},\n",
       " {'text': 'Prior to 2017, most language models would process text one word at a time,',\n",
       "  'start': 272.08,\n",
       "  'duration': 4.737},\n",
       " {'text': 'but then a team of researchers at Google introduced a new model known as the transformer.',\n",
       "  'start': 276.817,\n",
       "  'duration': 5.623},\n",
       " {'text': \"Transformers don't read text from the start to the finish,\",\n",
       "  'start': 283.3,\n",
       "  'duration': 3.445},\n",
       " {'text': 'they soak it all in at once, in parallel.',\n",
       "  'start': 286.745,\n",
       "  'duration': 2.395},\n",
       " {'text': 'The very first step inside a transformer, and most other language models for that matter,',\n",
       "  'start': 289.9,\n",
       "  'duration': 4.7},\n",
       " {'text': 'is to associate each word with a long list of numbers.',\n",
       "  'start': 294.6,\n",
       "  'duration': 2.82},\n",
       " {'text': 'The reason for this is that the training process only works with continuous values,',\n",
       "  'start': 297.86,\n",
       "  'duration': 4.536},\n",
       " {'text': 'so you have to somehow encode language using numbers,',\n",
       "  'start': 302.396,\n",
       "  'duration': 2.916},\n",
       " {'text': 'and each of these lists of numbers may somehow encode the meaning of the',\n",
       "  'start': 305.312,\n",
       "  'duration': 3.942},\n",
       " {'text': 'corresponding word.', 'start': 309.254, 'duration': 1.026},\n",
       " {'text': 'What makes transformers unique is their reliance',\n",
       "  'start': 310.28,\n",
       "  'duration': 3.08},\n",
       " {'text': 'on a special operation known as attention.',\n",
       "  'start': 313.36,\n",
       "  'duration': 2.64},\n",
       " {'text': 'This operation gives all of these lists of numbers a chance to talk to one another',\n",
       "  'start': 316.98,\n",
       "  'duration': 4.704},\n",
       " {'text': 'and refine the meanings they encode based on the context around, all done in parallel.',\n",
       "  'start': 321.684,\n",
       "  'duration': 4.876},\n",
       " {'text': 'For example, the numbers encoding the word bank might be changed based on the',\n",
       "  'start': 327.4,\n",
       "  'duration': 4.307},\n",
       " {'text': 'context surrounding it to somehow encode the more specific notion of a riverbank.',\n",
       "  'start': 331.707,\n",
       "  'duration': 4.473},\n",
       " {'text': 'Transformers typically also include a second type of operation known',\n",
       "  'start': 337.28,\n",
       "  'duration': 3.749},\n",
       " {'text': 'as a feed-forward neural network, and this gives the model extra',\n",
       "  'start': 341.029,\n",
       "  'duration': 3.532},\n",
       " {'text': 'capacity to store more patterns about language learned during training.',\n",
       "  'start': 344.561,\n",
       "  'duration': 3.859},\n",
       " {'text': 'All of this data repeatedly flows through many different iterations of',\n",
       "  'start': 349.28,\n",
       "  'duration': 4.121},\n",
       " {'text': 'these two fundamental operations, and as it does so,',\n",
       "  'start': 353.401,\n",
       "  'duration': 3.077},\n",
       " {'text': 'the hope is that each list of numbers is enriched to encode whatever',\n",
       "  'start': 356.478,\n",
       "  'duration': 4.006},\n",
       " {'text': 'information might be needed to make an accurate prediction of what word',\n",
       "  'start': 360.484,\n",
       "  'duration': 4.18},\n",
       " {'text': 'follows in the passage.', 'start': 364.664, 'duration': 1.336},\n",
       " {'text': 'At the end, one final function is performed on the last vector in this sequence,',\n",
       "  'start': 367.0,\n",
       "  'duration': 4.534},\n",
       " {'text': 'which now has had a chance to be influenced by all the other context from the input text,',\n",
       "  'start': 371.534,\n",
       "  'duration': 5.039},\n",
       " {'text': 'as well as everything the model learned during training,',\n",
       "  'start': 376.573,\n",
       "  'duration': 3.191},\n",
       " {'text': 'to produce a prediction of the next word.',\n",
       "  'start': 379.764,\n",
       "  'duration': 2.296},\n",
       " {'text': \"Again, the model's prediction looks like a probability for every possible next word.\",\n",
       "  'start': 382.48,\n",
       "  'duration': 4.88},\n",
       " {'text': 'Although researchers design the framework for how each of these steps work,',\n",
       "  'start': 388.56,\n",
       "  'duration': 4.234},\n",
       " {'text': \"it's important to understand that the specific behavior is an emergent phenomenon\",\n",
       "  'start': 392.794,\n",
       "  'duration': 4.568},\n",
       " {'text': 'based on how those hundreds of billions of parameters are tuned during training.',\n",
       "  'start': 397.362,\n",
       "  'duration': 4.458},\n",
       " {'text': 'This makes it incredibly challenging to determine',\n",
       "  'start': 402.48,\n",
       "  'duration': 2.59},\n",
       " {'text': 'why the model makes the exact predictions that it does.',\n",
       "  'start': 405.07,\n",
       "  'duration': 2.85},\n",
       " {'text': 'What you can see is that when you use large language model predictions to autocomplete',\n",
       "  'start': 408.44,\n",
       "  'duration': 5.338},\n",
       " {'text': 'a prompt, the words that it generates are uncannily fluent, fascinating, and even useful.',\n",
       "  'start': 413.778,\n",
       "  'duration': 5.462},\n",
       " {'text': \"If you're a new viewer and you're curious about more details on how\",\n",
       "  'start': 425.719,\n",
       "  'duration': 3.107},\n",
       " {'text': 'transformers and attention work, boy do I have some material for you.',\n",
       "  'start': 428.826,\n",
       "  'duration': 3.153},\n",
       " {'text': 'One option is to jump into a series I made about deep learning,',\n",
       "  'start': 432.399,\n",
       "  'duration': 3.681},\n",
       " {'text': 'where we visualize and motivate the details of attention and all the other steps',\n",
       "  'start': 436.08,\n",
       "  'duration': 4.66},\n",
       " {'text': 'in a transformer.', 'start': 440.74, 'duration': 0.979},\n",
       " {'text': 'Also, on my second channel I just posted a talk I gave a couple',\n",
       "  'start': 442.099,\n",
       "  'duration': 3.43},\n",
       " {'text': 'months ago about this topic for the company TNG in Munich.',\n",
       "  'start': 445.529,\n",
       "  'duration': 3.11},\n",
       " {'text': 'Sometimes I actually prefer the content I make as a casual talk rather than a produced',\n",
       "  'start': 449.079,\n",
       "  'duration': 4.022},\n",
       " {'text': 'video, but I leave it up to you which one of these feels like the better follow-on.',\n",
       "  'start': 453.101,\n",
       "  'duration': 3.838}]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9ec47c-e9a1-464c-81ca-c51974b91e39",
   "metadata": {},
   "source": [
    "### Step 1b - Indexing (Text Splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d7150334-6429-47e6-bd76-e620f43f6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap=200)\n",
    "chunks = splitter.create_documents([transcript])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a19586c5-de91-4e64-b91d-0a5780c37848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5c242ed1-20b8-4a66-b9c7-a3c34140e419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content=\"Imagine you happen across a short movie script that describes a scene between a person and their AI assistant. The script has what the person asks the AI, but the AI's response has been torn off. Suppose you also have this powerful magical machine that can take any text and provide a sensible prediction of what word comes next. You could then finish the script by feeding in what you have to the machine, seeing what it would predict to start the AI's answer, and then repeating this over and over with a growing script completing the dialogue. When you interact with a chatbot, this is exactly what's happening. A large language model is a sophisticated mathematical function that predicts what word comes next for any piece of text. Instead of predicting one word with certainty, though, what it does is assign a probability to all possible next words. To build a chatbot, you lay out some text that describes an interaction between a user and a hypothetical AI assistant, add on whatever the\")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c2fc8-4736-4ebd-95bd-38b62d10a8b3",
   "metadata": {},
   "source": [
    "### Step 1c & 1d - Indexing (Embedding Generation and Storing in Vector Store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "691e815d-8a2c-47c7-b831-456425d9bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    " \n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "96a4b41d-a148-4bd4-8fe9-fed54b6b756d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '2ed0042e-d136-43cc-95e3-edda41b98b6e',\n",
       " 1: '39eba529-09d3-4877-9cf8-2e12f2610ed4',\n",
       " 2: 'ee7649ec-57b2-4e82-8b3d-044df73aee41',\n",
       " 3: 'd3eae279-b15d-4808-95dd-45be93524a9e',\n",
       " 4: 'a7bf10b1-4b44-43a7-9d60-c09c9f2be434',\n",
       " 5: 'c7cb9326-ba35-4b69-b385-4a043ce4e621',\n",
       " 6: 'fa44abd1-facf-4e2a-b1f3-f71f08f779f7',\n",
       " 7: '5fa42f39-1efd-4f22-91bb-b28bb323af09',\n",
       " 8: '7f5f9f62-9d09-4779-87c2-1e9c2a93bf36',\n",
       " 9: '502c6bb7-f2c7-4bfa-b6f4-50380d72daae'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9f71dd08-4453-4094-9df7-a36ad1edd309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='502c6bb7-f2c7-4bfa-b6f4-50380d72daae', metadata={}, page_content='the other steps in a transformer. Also, on my second channel I just posted a talk I gave a couple months ago about this topic for the company TNG in Munich. Sometimes I actually prefer the content I make as a casual talk rather than a produced video, but I leave it up to you which one of these feels like the better follow-on.')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.get_by_ids(['502c6bb7-f2c7-4bfa-b6f4-50380d72daae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc8c9aa-52af-4898-af74-6219fbfbb8e0",
   "metadata": {},
   "source": [
    "### Step 2 - Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "828e878b-a2e9-4eee-81e0-5871e4c9684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7ff3bab9-f062-44bc-a221-1d5286409f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002857D0F7D10>, search_kwargs={'k': 4})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a02b2e90-84f2-4a94-99b0-41abdfcd5ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='2ed0042e-d136-43cc-95e3-edda41b98b6e', metadata={}, page_content=\"Imagine you happen across a short movie script that describes a scene between a person and their AI assistant. The script has what the person asks the AI, but the AI's response has been torn off. Suppose you also have this powerful magical machine that can take any text and provide a sensible prediction of what word comes next. You could then finish the script by feeding in what you have to the machine, seeing what it would predict to start the AI's answer, and then repeating this over and over with a growing script completing the dialogue. When you interact with a chatbot, this is exactly what's happening. A large language model is a sophisticated mathematical function that predicts what word comes next for any piece of text. Instead of predicting one word with certainty, though, what it does is assign a probability to all possible next words. To build a chatbot, you lay out some text that describes an interaction between a user and a hypothetical AI assistant, add on whatever the\"),\n",
       " Document(id='5fa42f39-1efd-4f22-91bb-b28bb323af09', metadata={}, page_content=\"type of operation known as a feed-forward neural network, and this gives the model extra capacity to store more patterns about language learned during training. All of this data repeatedly flows through many different iterations of these two fundamental operations, and as it does so, the hope is that each list of numbers is enriched to encode whatever information might be needed to make an accurate prediction of what word follows in the passage. At the end, one final function is performed on the last vector in this sequence, which now has had a chance to be influenced by all the other context from the input text, as well as everything the model learned during training, to produce a prediction of the next word. Again, the model's prediction looks like a probability for every possible next word. Although researchers design the framework for how each of these steps work, it's important to understand that the specific behavior is an emergent phenomenon based on how those hundreds of\"),\n",
       " Document(id='c7cb9326-ba35-4b69-b385-4a043ce4e621', metadata={}, page_content=\"called reinforcement learning with human feedback. Workers flag unhelpful or problematic predictions, and their corrections further change the model's parameters, making them more likely to give predictions that users prefer. Looking back at the pre-training, though, this staggering amount of computation is only made possible by using special computer chips that are optimized for running many operations in parallel, known as GPUs. However, not all language models can be easily parallelized. Prior to 2017, most language models would process text one word at a time, but then a team of researchers at Google introduced a new model known as the transformer. Transformers don't read text from the start to the finish, they soak it all in at once, in parallel. The very first step inside a transformer, and most other language models for that matter, is to associate each word with a long list of numbers. The reason for this is that the training process only works with continuous values, so you\"),\n",
       " Document(id='7f5f9f62-9d09-4779-87c2-1e9c2a93bf36', metadata={}, page_content=\"next word. Although researchers design the framework for how each of these steps work, it's important to understand that the specific behavior is an emergent phenomenon based on how those hundreds of billions of parameters are tuned during training. This makes it incredibly challenging to determine why the model makes the exact predictions that it does. What you can see is that when you use large language model predictions to autocomplete a prompt, the words that it generates are uncannily fluent, fascinating, and even useful. If you're a new viewer and you're curious about more details on how transformers and attention work, boy do I have some material for you. One option is to jump into a series I made about deep learning, where we visualize and motivate the details of attention and all the other steps in a transformer. Also, on my second channel I just posted a talk I gave a couple months ago about this topic for the company TNG in Munich. Sometimes I actually prefer the content I\")]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke('What is deepmind?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3607d7cf-4468-437f-ae14-fcd4c2ab6de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# till now we take query, pass to the retriever and perform semantic search,\n",
    "# get the most relevant context and pass further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e05598-8e18-4cbb-ad27-8c78dea28d10",
   "metadata": {},
   "source": [
    "### Step 3 - Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "aa031b92-ee0f-41ca-bfaa-48c369605a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template = \"\"\"\n",
    "    You are a helpful assistant.\n",
    "    Answer ONLY from the provided transcript context.\n",
    "    If the context is insufficient, just say you don't know.\n",
    "\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\",\n",
    "    input_variables = ['context', 'question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f3e2111f-d213-420f-9275-8fa710667bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"is the topic of LLM in this video? If yes, then what was that?\"\n",
    "retrieved_docs = retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ed29796e-83a9-42f8-8b49-2c823783c145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='502c6bb7-f2c7-4bfa-b6f4-50380d72daae', metadata={}, page_content='the other steps in a transformer. Also, on my second channel I just posted a talk I gave a couple months ago about this topic for the company TNG in Munich. Sometimes I actually prefer the content I make as a casual talk rather than a produced video, but I leave it up to you which one of these feels like the better follow-on.'),\n",
       " Document(id='5fa42f39-1efd-4f22-91bb-b28bb323af09', metadata={}, page_content=\"type of operation known as a feed-forward neural network, and this gives the model extra capacity to store more patterns about language learned during training. All of this data repeatedly flows through many different iterations of these two fundamental operations, and as it does so, the hope is that each list of numbers is enriched to encode whatever information might be needed to make an accurate prediction of what word follows in the passage. At the end, one final function is performed on the last vector in this sequence, which now has had a chance to be influenced by all the other context from the input text, as well as everything the model learned during training, to produce a prediction of the next word. Again, the model's prediction looks like a probability for every possible next word. Although researchers design the framework for how each of these steps work, it's important to understand that the specific behavior is an emergent phenomenon based on how those hundreds of\"),\n",
       " Document(id='7f5f9f62-9d09-4779-87c2-1e9c2a93bf36', metadata={}, page_content=\"next word. Although researchers design the framework for how each of these steps work, it's important to understand that the specific behavior is an emergent phenomenon based on how those hundreds of billions of parameters are tuned during training. This makes it incredibly challenging to determine why the model makes the exact predictions that it does. What you can see is that when you use large language model predictions to autocomplete a prompt, the words that it generates are uncannily fluent, fascinating, and even useful. If you're a new viewer and you're curious about more details on how transformers and attention work, boy do I have some material for you. One option is to jump into a series I made about deep learning, where we visualize and motivate the details of attention and all the other steps in a transformer. Also, on my second channel I just posted a talk I gave a couple months ago about this topic for the company TNG in Munich. Sometimes I actually prefer the content I\"),\n",
       " Document(id='2ed0042e-d136-43cc-95e3-edda41b98b6e', metadata={}, page_content=\"Imagine you happen across a short movie script that describes a scene between a person and their AI assistant. The script has what the person asks the AI, but the AI's response has been torn off. Suppose you also have this powerful magical machine that can take any text and provide a sensible prediction of what word comes next. You could then finish the script by feeding in what you have to the machine, seeing what it would predict to start the AI's answer, and then repeating this over and over with a growing script completing the dialogue. When you interact with a chatbot, this is exactly what's happening. A large language model is a sophisticated mathematical function that predicts what word comes next for any piece of text. Instead of predicting one word with certainty, though, what it does is assign a probability to all possible next words. To build a chatbot, you lay out some text that describes an interaction between a user and a hypothetical AI assistant, add on whatever the\")]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "40f43a40-35c9-4c9e-b57f-ddf59c814deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b59eea7f-ace8-47be-92e0-390c24fa8493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the other steps in a transformer. Also, on my second channel I just posted a talk I gave a couple months ago about this topic for the company TNG in Munich. Sometimes I actually prefer the content I make as a casual talk rather than a produced video, but I leave it up to you which one of these feels like the better follow-on.\\n\\ntype of operation known as a feed-forward neural network, and this gives the model extra capacity to store more patterns about language learned during training. All of this data repeatedly flows through many different iterations of these two fundamental operations, and as it does so, the hope is that each list of numbers is enriched to encode whatever information might be needed to make an accurate prediction of what word follows in the passage. At the end, one final function is performed on the last vector in this sequence, which now has had a chance to be influenced by all the other context from the input text, as well as everything the model learned during training, to produce a prediction of the next word. Again, the model's prediction looks like a probability for every possible next word. Although researchers design the framework for how each of these steps work, it's important to understand that the specific behavior is an emergent phenomenon based on how those hundreds of\\n\\nnext word. Although researchers design the framework for how each of these steps work, it's important to understand that the specific behavior is an emergent phenomenon based on how those hundreds of billions of parameters are tuned during training. This makes it incredibly challenging to determine why the model makes the exact predictions that it does. What you can see is that when you use large language model predictions to autocomplete a prompt, the words that it generates are uncannily fluent, fascinating, and even useful. If you're a new viewer and you're curious about more details on how transformers and attention work, boy do I have some material for you. One option is to jump into a series I made about deep learning, where we visualize and motivate the details of attention and all the other steps in a transformer. Also, on my second channel I just posted a talk I gave a couple months ago about this topic for the company TNG in Munich. Sometimes I actually prefer the content I\\n\\nImagine you happen across a short movie script that describes a scene between a person and their AI assistant. The script has what the person asks the AI, but the AI's response has been torn off. Suppose you also have this powerful magical machine that can take any text and provide a sensible prediction of what word comes next. You could then finish the script by feeding in what you have to the machine, seeing what it would predict to start the AI's answer, and then repeating this over and over with a growing script completing the dialogue. When you interact with a chatbot, this is exactly what's happening. A large language model is a sophisticated mathematical function that predicts what word comes next for any piece of text. Instead of predicting one word with certainty, though, what it does is assign a probability to all possible next words. To build a chatbot, you lay out some text that describes an interaction between a user and a hypothetical AI assistant, add on whatever the\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "05e2b87a-d952-41c9-8e07-5a98febb3aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = prompt.invoke({\"context\":context_text, \"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1479be4c-3f28-4ee4-bddf-116c3a88286d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"\\n    You are a helpful assistant.\\n    Answer ONLY from the provided transcript context.\\n    If the context is insufficient, just say you don't know.\\n\\n    the other steps in a transformer. Also, on my second channel I just posted a talk I gave a couple months ago about this topic for the company TNG in Munich. Sometimes I actually prefer the content I make as a casual talk rather than a produced video, but I leave it up to you which one of these feels like the better follow-on.\\n\\ntype of operation known as a feed-forward neural network, and this gives the model extra capacity to store more patterns about language learned during training. All of this data repeatedly flows through many different iterations of these two fundamental operations, and as it does so, the hope is that each list of numbers is enriched to encode whatever information might be needed to make an accurate prediction of what word follows in the passage. At the end, one final function is performed on the last vector in this sequence, which now has had a chance to be influenced by all the other context from the input text, as well as everything the model learned during training, to produce a prediction of the next word. Again, the model's prediction looks like a probability for every possible next word. Although researchers design the framework for how each of these steps work, it's important to understand that the specific behavior is an emergent phenomenon based on how those hundreds of\\n\\nnext word. Although researchers design the framework for how each of these steps work, it's important to understand that the specific behavior is an emergent phenomenon based on how those hundreds of billions of parameters are tuned during training. This makes it incredibly challenging to determine why the model makes the exact predictions that it does. What you can see is that when you use large language model predictions to autocomplete a prompt, the words that it generates are uncannily fluent, fascinating, and even useful. If you're a new viewer and you're curious about more details on how transformers and attention work, boy do I have some material for you. One option is to jump into a series I made about deep learning, where we visualize and motivate the details of attention and all the other steps in a transformer. Also, on my second channel I just posted a talk I gave a couple months ago about this topic for the company TNG in Munich. Sometimes I actually prefer the content I\\n\\nImagine you happen across a short movie script that describes a scene between a person and their AI assistant. The script has what the person asks the AI, but the AI's response has been torn off. Suppose you also have this powerful magical machine that can take any text and provide a sensible prediction of what word comes next. You could then finish the script by feeding in what you have to the machine, seeing what it would predict to start the AI's answer, and then repeating this over and over with a growing script completing the dialogue. When you interact with a chatbot, this is exactly what's happening. A large language model is a sophisticated mathematical function that predicts what word comes next for any piece of text. Instead of predicting one word with certainty, though, what it does is assign a probability to all possible next words. To build a chatbot, you lay out some text that describes an interaction between a user and a hypothetical AI assistant, add on whatever the\\n    Question: is the topic of LLM in this video? If yes, then what was that?\\n    \")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b2867a-f6a4-42f4-a12e-594358051db7",
   "metadata": {},
   "source": [
    "### Step 4 - Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b13c2875-5f87-4eef-bc78-9d16cf807e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Answer: Yes, the topic of large language models (LLMs) is discussed in this video. The video explains how LLMs work by repeatedly flowing data through multiple iterations of two fundamental operations called self-attention and feed-forward neural networks. The video also highlights the emergent behavior of LLMs based on how their hundreds of billions of parameters are tuned during training, making it challenging to determine why the model makes specific predictions. The video suggests that LLMs can generate fluent, fascinating, and even useful predictions when used to autocomplete prompts. The video also recommends resources for viewers who want to learn more about LLMs and transformers, including a series on deep learning and a talk given by the author for a company called TNG in Munich.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(model=\"HuggingFaceH4/zephyr-7b-beta\")   \n",
    "\n",
    "# Call the model with the plain text from the StringPromptValue\n",
    "response = client.text_generation(\n",
    "    prompt= final_prompt.text,  # .text gives the raw string\n",
    "    max_new_tokens=200\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a13888c-3bde-4a18-8d38-6c51fc0b8190",
   "metadata": {},
   "source": [
    "### Building a Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f6e30f7d-936b-4236-80c8-c59a180d228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "13b622f3-a8b7-44d1-9ea7-992c91ee5527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(retrieved_docs):\n",
    "    context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    return context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b2102015-167c-4fed-b5fd-03d8c4dda54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain = RunnableParallel({\n",
    "    'context': retriever | RunnableLambda(format_docs), # as soon as it gets a question it passes to the retriever and then pass to the format_docs and return the string\n",
    "    'question': RunnablePassthrough()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4c77463c-fa88-486c-b818-6867208d1cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parallel_chain.invoke('Why llm needs to learn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c7d85632-3dbb-4d84-be35-045c784f9653",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_client = InferenceClient(model=\"HuggingFaceH4/zephyr-7b-beta\")   \n",
    "\n",
    "from langchain_core.runnables import Runnable\n",
    "from typing import Any, Dict\n",
    "\n",
    "class HuggingFaceTextGenRunnable(Runnable):\n",
    "    def __init__(self, client: InferenceClient):\n",
    "        self.client = client\n",
    "\n",
    "    def invoke(self, input: str, config: Dict[str, Any] = None) -> str:\n",
    "        response = self.client.text_generation(\n",
    "            prompt=input.text, \n",
    "            max_new_tokens=200\n",
    "        )\n",
    "        return response\n",
    "        \n",
    "client_runnable = HuggingFaceTextGenRunnable(hf_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ffe91a7b-2b22-4f76-9b89-12f88412251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c9f1aa4c-74ec-48fa-be29-bbf769b31583",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_chain = parallel_chain | prompt | client_runnable | parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2dc5f171-8059-4a67-a920-fedf769b1f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Answer: Large Language Models (LLMs) are a type of artificial intelligence (AI) technology that can understand and generate human-like text. They are trained on vast amounts of text data, allowing them to make predictions about the next word in a sentence or the meaning of a phrase. LLMs have a wide range of applications, from improving search engine results and generating personalized content to assisting with language learning and providing customer support. As the amount of text data available continues to grow, LLMs are becoming increasingly important in many industries, including healthcare, finance, and education. Therefore, learning LLMs is essential for anyone interested in staying up-to-date with the latest developments in AI and understanding its potential impact on various fields.'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_chain.invoke('Why do we need to learn llm?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cf15dedd-9d8c-499f-8860-8bf7adc695d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    Answer: The video explains how large language models, specifically transformers, predict the next word in a text. It highlights the two fundamental operations in a transformer, self-attention and feed-forward neural networks, and how they repeatedly flow through the model to enrich the list of numbers representing the input text. The video also mentions the emergent phenomenon of the model's behavior based on how the hundreds of billions of parameters are tuned during training. The video suggests watching a series on deep learning to visualize and motivate the details of attention and all the other steps in a transformer. The video also mentions a talk given by the speaker for the company TNG in Munich, which can be found on their second channel. The video concludes by explaining how a large language model predicts what word comes next for any piece of text, assigning a probability to all possible next words, and how this is used to build chatbots by completing\""
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_chain.invoke('Can you summarize the video?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3028983c-22a5-4df5-918a-11a8a7e552d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
